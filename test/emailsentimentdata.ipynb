{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import dtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4901, 9)\n"
     ]
    }
   ],
   "source": [
    "file_path = r'data\\EMAILSAMPLEDATA.parquet'\n",
    "df = pd.read_parquet(file_path)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dtale.show(df)\n",
    "# d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tabula-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>from_email</th>\n",
       "      <th>to_email</th>\n",
       "      <th>cc_email</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>date_ist</th>\n",
       "      <th>time_ist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15d7e872-3ed2-4d41-b6a0-515e7b0effdf</td>\n",
       "      <td>EhudR@afcon.co.il</td>\n",
       "      <td>soumendrar@pinnacleinfotech.com</td>\n",
       "      <td>Joel.adonaBimspecialistphils@outlook.com, Siva...</td>\n",
       "      <td>RE: IFF0203</td>\n",
       "      <td>Thu, 25 Apr 2024 12:38:36 +0000</td>\n",
       "      <td>1B-1-TB/11-050,051 tack example for wall surfa...</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>18:08:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48d1d13f-28f8-4db2-87e4-ac9125ee812c</td>\n",
       "      <td>Dcaf@tatatel.co.in</td>\n",
       "      <td>SUDIPR@PINNACLEINFOTECH.COM</td>\n",
       "      <td>krishnadas.poddar.kdb@tatatel.co.in, SUDIP.SAH...</td>\n",
       "      <td>Your CAF WBG-8631700017209 and Company Name : ...</td>\n",
       "      <td>Tue, 23 Apr 2024 14:48:30 +0000 (UTC)</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;\\n\\t&lt;title&gt;9_SLA mailer-01&lt;/titl...</td>\n",
       "      <td>2024-04-23</td>\n",
       "      <td>20:18:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id          from_email  \\\n",
       "0  15d7e872-3ed2-4d41-b6a0-515e7b0effdf   EhudR@afcon.co.il   \n",
       "1  48d1d13f-28f8-4db2-87e4-ac9125ee812c  Dcaf@tatatel.co.in   \n",
       "\n",
       "                          to_email  \\\n",
       "0  soumendrar@pinnacleinfotech.com   \n",
       "1      SUDIPR@PINNACLEINFOTECH.COM   \n",
       "\n",
       "                                            cc_email  \\\n",
       "0  Joel.adonaBimspecialistphils@outlook.com, Siva...   \n",
       "1  krishnadas.poddar.kdb@tatatel.co.in, SUDIP.SAH...   \n",
       "\n",
       "                                             subject  \\\n",
       "0                                        RE: IFF0203   \n",
       "1  Your CAF WBG-8631700017209 and Company Name : ...   \n",
       "\n",
       "                                    date  \\\n",
       "0        Thu, 25 Apr 2024 12:38:36 +0000   \n",
       "1  Tue, 23 Apr 2024 14:48:30 +0000 (UTC)   \n",
       "\n",
       "                                                body    date_ist  time_ist  \n",
       "0  1B-1-TB/11-050,051 tack example for wall surfa...  2024-04-25  18:08:46  \n",
       "1  <html>\\n<head>\\n\\t<title>9_SLA mailer-01</titl...  2024-04-23  20:18:36  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_disclaimer_text(body):\n",
    "    \"\"\"\n",
    "    Remove disclaimer and classification notices from the email body.\n",
    "\n",
    "    Args:\n",
    "        body (str): The body of the email.\n",
    "        \n",
    "    Returns:\n",
    "        str: The email body without disclaimer and classification notices.\n",
    "    \"\"\"\n",
    "    disclaimer_pattern = [\n",
    "        r\"\\*This e-mail has been classified as.*\",\n",
    "        r\".*classification notice.*\",\n",
    "        r\"From:\\s.*Sent:\\s.*To:\\s.*\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in disclaimer_pattern:\n",
    "        body = re.sub(pattern, '', body, flags=re.DOTALL)\n",
    "    \n",
    "    return body\n",
    "\n",
    "def extract_recent_email(email_body):\n",
    "    \"\"\"\n",
    "    Extract the most recent part of an email thread above the signature or prior email content.\n",
    "\n",
    "    Args:\n",
    "        email_body (str): The body of the email.\n",
    "        \n",
    "    Returns:\n",
    "        str: The most recent part text.\n",
    "    \"\"\"\n",
    "    # Use BeautifulSoup to handle HTML email bodies.\n",
    "    soup = BeautifulSoup(email_body, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Regex patterns for detecting common signature lines and previous messages\n",
    "    signature_patterns = [\n",
    "        re.compile(r'^(-{2,}|_{2,})'),  # Signatures, like \"-----Original Message-----\" or \"__\"\n",
    "        re.compile(r'^\\s*(Sent from my [\\w\\s]+|Sent with [\\w\\s]+)$', re.IGNORECASE), # Mobile signatures\n",
    "        re.compile(r'^On \\d{4}/[0-1]\\d/[0-3]\\d,.*'),  # Replies, like \"On 2021/12/01, ... wrote:\"\n",
    "        re.compile(r'^\\s*From:\\s*.*', re.IGNORECASE), # From header\n",
    "        re.compile(r'^\\s*Sent:\\s*.*', re.IGNORECASE), # Sent header\n",
    "        re.compile(r'^To:\\s*.*', re.IGNORECASE),      # To header\n",
    "        re.compile(r'^Subject:\\s*.*', re.IGNORECASE), # Subject header\n",
    "    ]\n",
    "\n",
    "    # Find the position of signature or previous message markers\n",
    "    for i, line in enumerate(lines):\n",
    "        for pattern in signature_patterns:\n",
    "            if pattern.match(line):\n",
    "                return \"\\n\".join(lines[:i]).strip()  # Return everything before the signature marker\n",
    "\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "def extract_text_above_signature(body, from_email):\n",
    "    \"\"\"\n",
    "    Extract text above the signature in the email body.\n",
    "\n",
    "    Args:\n",
    "        body (str): The body of the email.\n",
    "        from_email (str): The sender's email address.\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted text above signature patterns.\n",
    "    \"\"\"\n",
    "    username = from_email.split('@')[0]  # Extract the username\n",
    "    signature_patterns = [\n",
    "        r\"\\nfrom:\\s\", r\"\\nsent:\\s\", r\"\\nto:\\s\", r\"\\ncc:\\s\", r\"\\nsubject:\\s\",\n",
    "        r\"\\nregards,\\s\", r\"\\nbest,\\s\", r\"\\nthanks,\\s\", r\"\\nsincerely,\\s\", r\"\\ncheers,\\s\",\n",
    "        re.escape(username.lower())\n",
    "    ]\n",
    "    \n",
    "    body_lower = body.lower()\n",
    "    signature_indices = []\n",
    "    \n",
    "    # Find all occurrences of the signature patterns\n",
    "    for pattern in signature_patterns:\n",
    "        matches = list(re.finditer(pattern, body_lower, re.MULTILINE))\n",
    "        signature_indices.extend([match.start() for match in matches])\n",
    "    \n",
    "    if signature_indices:\n",
    "        signature_index = min(signature_indices)\n",
    "        return body[:signature_index].strip()\n",
    "    return body\n",
    "\n",
    "def clean_email_body(body):\n",
    "    \"\"\"\n",
    "    Clean the email body by removing unwanted text based on various regex patterns.\n",
    "\n",
    "    Args:\n",
    "        body (str): The body of the email.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned email body.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'urls': r'https?://\\S+|www\\.\\S+',  # URLs\n",
    "        'metadata': r'(?m)^(From|Sent|To|Cc|Bcc|Subject|Date): .*$',  # Email metadata\n",
    "        'greetings': r'(?i)^(Hi|Hello|Dear|Greetings|Hey)\\s+[^\\n]+',  # Greetings\n",
    "        'signatures': r'(?i)(Best regards|Kind regards|Regards|Cheers|Thank you|Thanks|Sincerely|Yours truly|Yours sincerely|Best|Warm regards|With regards)[^\\n]+',  # Signatures\n",
    "        'email_headers': r'---* Forwarded message ---*|---* Original message ---*|---* Reply Above This Line ---*',  # Email forwarding/reply headers\n",
    "        'email_addresses': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email addresses\n",
    "        'non_ascii': r'[^\\x00-\\x7F]+',  # Non-ASCII characters\n",
    "        'extra_lines': r'\\n{2,}',  # Excessive newlines\n",
    "        'html_tags': r'<[^>]*>',  # HTML tags\n",
    "        'brackets_content': r'\\[.*?\\]|\\{.*?\\}|\\<.*?\\>',  # Content within brackets\n",
    "        'extra_whitespace': r'\\s{2,}',  # Excessive whitespace\n",
    "        'unsubscribe_links': r'Unsubscribe\\s+:.*|Click\\s+here.*unsubscribe',  # Unsubscribe links\n",
    "        'reply_lines': r'On\\s+.*wrote:',  # Lines indicating the start of a reply\n",
    "        'quoted_text': r'(?m)^\\>.*$',  # Lines starting with \">\"\n",
    "        'repeated_chars': r'(.)\\1{2,}',  # Repeated characters\n",
    "        'generic_signature_lines': r'(?i)(^Sent from my \\w+)|(--\\n.*)|(Confidentiality Notice.*)',  # Generic email signatures or legal disclaimers\n",
    "        'timestamps': r'\\d{1,2}:\\d{2}\\s*(AM|PM|am|pm)?',  # Time stamps\n",
    "        'dates': r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}',  # Dates\n",
    "        'signature_blocks': r'--\\s*\\n[\\s\\S]*',  # Signature blocks that start with -- followed by any text\n",
    "        'headers_and_footers': r'(?m)^\\s*-\\s*$\\n[\\s\\S]*?^\\s*-\\s*$',  # Headers and footers denoted by lines consisting of dashes\n",
    "        'mobile_numbers': r'\\b(\\+?(\\d{1,3})?[-. ]?)?((\\(\\d{1,4}\\))|\\d{1,4})[-. ]?\\d{1,4}[-. ]?\\d{1,9}\\b'  # Mobile numbers\n",
    "    }\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        body = re.sub(pattern, ' ', body)\n",
    "    \n",
    "    # Remove excess whitespace\n",
    "    body = re.sub(r'\\s{2,}', ' ', body)\n",
    "    return body.strip()\n",
    "\n",
    "\n",
    "def extract_most_recent_email_part(email_body, from_email):\n",
    "    \"\"\"\n",
    "    Combine recent email extraction and signature pattern extraction for robust extraction.\n",
    "\n",
    "    Args:\n",
    "        email_body (str): The body of the email.\n",
    "        from_email (str): The sender's email address.\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted relevant part of the email above signatures and prior threads.\n",
    "    \"\"\"\n",
    "    # Remove disclaimer and notice sections\n",
    "    cleaned_body = remove_disclaimer_text(email_body)\n",
    "    \n",
    "    # Extract the most recent email section ignoring previous emails\n",
    "    recent_email_content = extract_recent_email(cleaned_body)\n",
    "    \n",
    "    # Extract text above signatures and replies\n",
    "    filtered_content_above_signature = extract_text_above_signature(recent_email_content, from_email)\n",
    "\n",
    "    # Clean the email body\n",
    "    cleaned_body = clean_email_body(filtered_content_above_signature)\n",
    "    \n",
    "    return filtered_content_above_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = df.head(5).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pis05408.PINNACLE\\AppData\\Local\\Temp\\ipykernel_24332\\449057951.py:33: MarkupResemblesLocatorWarning:\n",
      "\n",
      "The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Apply the extraction function\n",
    "# test['clean_email'] = test['body'].apply(extract_recent_email)\n",
    "\n",
    "# Apply the extraction function\n",
    "df['clean_email'] = df.apply(lambda row: extract_most_recent_email_part(row['body'], row['from_email']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Sir/Madam,\n",
      "\n",
      "Greetings! I hope this message finds you well. I am\n"
     ]
    }
   ],
   "source": [
    "# Display the result\n",
    "print(df['clean_email'][70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dtale.show(test)\n",
    "# d.open_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define the sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to split text into chunks\n",
    "# def split_text_into_chunks(text, max_length=512):\n",
    "#     return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# # Function to predict sentiment on text chunks\n",
    "# def predict_sentiment_chunks(text):\n",
    "#     chunks = split_text_into_chunks(text)\n",
    "#     sentiments = [sentiment_pipeline(chunk)[0] for chunk in chunks]\n",
    "#     labels = [sent['label'] for sent in sentiments]\n",
    "#     scores = [sent['score'] for sent in sentiments]\n",
    "    \n",
    "#     # Majority vote for label\n",
    "#     label = max(set(labels), key=labels.count)\n",
    "    \n",
    "#     # Average score for the majority label\n",
    "#     score = sum([score for i, score in enumerate(scores) if labels[i] == label]) / labels.count(label)\n",
    "    \n",
    "#     return {'label': label, 'score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to predict sentiment with text truncation\n",
    "def predict_sentiment_truncated(text, max_length=512):\n",
    "    truncated_text = text[:max_length]\n",
    "    result = sentiment_pipeline(truncated_text)\n",
    "    return result[0] if result else {'label': 'UNKNOWN', 'score': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment prediction function\n",
    "df['sentiment_distilbert'] = df['clean_email'].apply(predict_sentiment_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sentiment label and score into separate columns\n",
    "df['sentiment_distilbert_label'] = df['sentiment_distilbert'].apply(lambda x: x['label'])\n",
    "df['sentiment_distilbert_score'] = df['sentiment_distilbert'].apply(lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'from_email', 'to_email', 'cc_email', 'subject', 'date', 'body',\n",
       "       'date_ist', 'time_ist', 'clean_email', 'sentiment_1',\n",
       "       'sentiment_transformer_label', 'sentiment_transformer_score',\n",
       "       'sentiment', 'sentiment_blob_level', 'sentiment_blob_score',\n",
       "       'sentiment_distilbert', 'sentiment_distilbert_label',\n",
       "       'sentiment_distilbert_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_distilbert_label\n",
       "NEGATIVE    3156\n",
       "POSITIVE    1745\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_distilbert_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dtale.show(df)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment using TextBlob\n",
    "def predict_sentiment_textblob(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment analysis function\n",
    "df['sentiment'] = df['clean_email'].apply(predict_sentiment_textblob)\n",
    "\n",
    "# Extract the sentiment label and score into separate columns\n",
    "df['sentiment_blob_level'] = df['sentiment'].apply(lambda x: 'POSITIVE' if x.polarity > 0 else ('NEGATIVE' if x.polarity < 0 else 'NEUTRAL'))\n",
    "df['sentiment_blob_score'] = df['sentiment'].apply(lambda x: x.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_blob_level\n",
       "POSITIVE    2924\n",
       "NEUTRAL     1278\n",
       "NEGATIVE     699\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment_blob_level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dtale.show(df)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flair.models import TextClassifier\n",
    "# from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\pis05408.PINNACLE\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n"
     ]
    }
   ],
   "source": [
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment using VADER\n",
    "def predict_sentiment_nltk(text):\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment analysis function\n",
    "df['sentiment_NLTK'] = df['clean_email'].apply(predict_sentiment_nltk)\n",
    "\n",
    "# Extract the sentiment label and score into separate columns\n",
    "df['sentiment_nltk_label'] = df['sentiment_NLTK'].apply(lambda x: 'POSITIVE' if x['compound'] >= 0.05 else ('NEGATIVE' if x['compound'] <= -0.05 else 'NEUTRAL'))\n",
    "df['sentiment_nltk_score'] = df['sentiment_NLTK'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_nltk_label\n",
       "POSITIVE    4009\n",
       "NEUTRAL      564\n",
       "NEGATIVE     328\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_nltk_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x22d2e011eb0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the TextBlob component to the SpaCy pipeline\n",
    "nlp.add_pipe(\"spacytextblob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment using SpaCy\n",
    "def predict_sentiment_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    sentiment = {\n",
    "        'label': 'POSITIVE' if doc._.polarity > 0 else ('NEGATIVE' if doc._.polarity < 0 else 'NEUTRAL'),\n",
    "        'score': doc._.polarity\n",
    "    }\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment analysis function\n",
    "df['sentiment_spacy'] = df['clean_email'].apply(predict_sentiment_spacy)\n",
    "\n",
    "# Extract the sentiment label and score into separate columns\n",
    "df['sentiment_spacy_label'] = df['sentiment_spacy'].apply(lambda x: x['label'])\n",
    "df['sentiment_spacy_score'] = df['sentiment_spacy'].apply(lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_spacy_label\n",
       "POSITIVE    2924\n",
       "NEUTRAL     1278\n",
       "NEGATIVE     699\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_spacy_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentiment analysis pipeline with a pre-trained RoBERTa model\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    result = sentiment_pipeline(text)\n",
    "    return result[0] if result else {'label': 'UNKNOWN', 'score': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\pis05408.PINNACLE\\AppData\\Local\\Temp\\ipykernel_24332\\538123373.py\", line 2, in <module>\n",
      "    df['sentiment_cardiffnlp'] = df['clean_email'].apply(predict_sentiment)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\pandas\\core\\series.py\", line 4753, in apply\n",
      "    axis=_shared_doc_kwargs[\"axis\"],\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\pandas\\core\\apply.py\", line 1207, in apply\n",
      "    return res\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\pandas\\core\\apply.py\", line 1287, in apply_standard\n",
      "    ) -> Callable[[npt.NDArray, Index, Index], dict[int, Any]]:\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\pandas\\core\\base.py\", line 921, in _map_values\n",
      "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\pandas\\core\\algorithms.py\", line 1814, in map_array\n",
      "  File \"lib.pyx\", line 2920, in pandas._libs.lib.map_infer\n",
      "  File \"C:\\Users\\pis05408.PINNACLE\\AppData\\Local\\Temp\\ipykernel_24332\\1793306454.py\", line 6, in predict_sentiment\n",
      "    result = sentiment_pipeline(text)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\pipelines\\text_classification.py\", line 156, in __call__\n",
      "    result = super().__call__(*inputs, **kwargs)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1243, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1250, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1150, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\pipelines\\text_classification.py\", line 187, in _forward\n",
      "    return self.model(**model_inputs)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 1195, in forward\n",
      "    outputs = self.roberta(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 798, in forward\n",
      "    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
      "RuntimeError: The expanded size of the tensor (1555) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 1555].  Tensor sizes: [1, 514]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Apply the sentiment analysis function\n",
    "df['sentiment_cardiffnlp'] = df['clean_email'].apply(predict_sentiment)\n",
    "\n",
    "# Extract the sentiment label and score into separate columns\n",
    "df['sentiment_label_cardiffnlp'] = df['sentiment_cardiffnlp'].apply(lambda x: x['label'])\n",
    "df['sentiment_score_cardiffnlp'] = df['sentiment_cardiffnlp'].apply(lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install allennlp allennlp-models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pis05408.PINNACLE\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pis05408.PINNACLE\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "2024-05-23 19:54:01,915 - INFO     - Plugin allennlp_models available\n",
      "2024-05-23 19:54:02,508 - INFO     - https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.02.17.tar.gz not found in cache, downloading to C:\\Users\\pis05408.PINNACLE\\.allennlp\\cache\\d4cca2cbc54d5b1ed6450dad2c2ee3df941d1f64b94ed7c29ee92b530ed32975.68c2d52dd7bf3cba019951262c2624faa0a0e4977a16c62b59167cb9247985f0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\rich\\live.py:229: UserWarning:\n",
       "\n",
       "install \"ipywidgets\" for Jupyter support\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\rich\\live.py:229: UserWarning:\n",
       "\n",
       "install \"ipywidgets\" for Jupyter support\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 19:56:06,746 - INFO     - loading archive file https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.02.17.tar.gz from cache at C:\\Users\\pis05408.PINNACLE\\.allennlp\\cache\\d4cca2cbc54d5b1ed6450dad2c2ee3df941d1f64b94ed7c29ee92b530ed32975.68c2d52dd7bf3cba019951262c2624faa0a0e4977a16c62b59167cb9247985f0\n",
      "2024-05-23 19:56:06,747 - INFO     - extracting archive file C:\\Users\\pis05408.PINNACLE\\.allennlp\\cache\\d4cca2cbc54d5b1ed6450dad2c2ee3df941d1f64b94ed7c29ee92b530ed32975.68c2d52dd7bf3cba019951262c2624faa0a0e4977a16c62b59167cb9247985f0 to temp dir C:\\Users\\PIS054~1.PIN\\AppData\\Local\\Temp\\tmp82t24yl3\n",
      "2024-05-23 19:56:14,765 - WARNING  - error loading _jsonnet (this is expected on Windows), treating C:\\Users\\PIS054~1.PIN\\AppData\\Local\\Temp\\tmp82t24yl3\\config.json as plain json\n",
      "2024-05-23 19:56:14,774 - INFO     - dataset_reader.type = sst_tokens\n",
      "2024-05-23 19:56:14,775 - INFO     - dataset_reader.max_instances = None\n",
      "2024-05-23 19:56:14,775 - INFO     - dataset_reader.manual_distributed_sharding = False\n",
      "2024-05-23 19:56:14,777 - INFO     - dataset_reader.manual_multiprocess_sharding = False\n",
      "2024-05-23 19:56:14,778 - INFO     - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "2024-05-23 19:56:14,779 - INFO     - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "2024-05-23 19:56:14,780 - INFO     - dataset_reader.token_indexers.tokens.model_name = roberta-large\n",
      "2024-05-23 19:56:14,780 - INFO     - dataset_reader.token_indexers.tokens.namespace = tags\n",
      "2024-05-23 19:56:14,781 - INFO     - dataset_reader.token_indexers.tokens.max_length = None\n",
      "2024-05-23 19:56:14,782 - INFO     - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n",
      "2024-05-23 19:56:21,430 - INFO     - dataset_reader.tokenizer = None\n",
      "2024-05-23 19:56:21,431 - INFO     - dataset_reader.use_subtrees = True\n",
      "2024-05-23 19:56:21,432 - INFO     - dataset_reader.granularity = 2-class\n",
      "2024-05-23 19:56:21,432 - INFO     - validation_dataset_reader.type = sst_tokens\n",
      "2024-05-23 19:56:21,433 - INFO     - validation_dataset_reader.max_instances = None\n",
      "2024-05-23 19:56:21,434 - INFO     - validation_dataset_reader.manual_distributed_sharding = False\n",
      "2024-05-23 19:56:21,434 - INFO     - validation_dataset_reader.manual_multiprocess_sharding = False\n",
      "2024-05-23 19:56:21,435 - INFO     - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "2024-05-23 19:56:21,436 - INFO     - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "2024-05-23 19:56:21,436 - INFO     - validation_dataset_reader.token_indexers.tokens.model_name = roberta-large\n",
      "2024-05-23 19:56:21,437 - INFO     - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "2024-05-23 19:56:21,438 - INFO     - validation_dataset_reader.token_indexers.tokens.max_length = None\n",
      "2024-05-23 19:56:21,438 - INFO     - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "2024-05-23 19:56:21,440 - INFO     - validation_dataset_reader.tokenizer = None\n",
      "2024-05-23 19:56:21,441 - INFO     - validation_dataset_reader.use_subtrees = False\n",
      "2024-05-23 19:56:21,441 - INFO     - validation_dataset_reader.granularity = 2-class\n",
      "2024-05-23 19:56:21,442 - INFO     - type = from_instances\n",
      "2024-05-23 19:56:21,443 - INFO     - Loading token dictionary from C:\\Users\\PIS054~1.PIN\\AppData\\Local\\Temp\\tmp82t24yl3\\vocabulary.\n",
      "2024-05-23 19:56:21,452 - INFO     - model.type = basic_classifier\n",
      "2024-05-23 19:56:21,453 - INFO     - model.regularizer = None\n",
      "2024-05-23 19:56:21,453 - INFO     - model.ddp_accelerator = None\n",
      "2024-05-23 19:56:21,454 - INFO     - model.text_field_embedder.type = basic\n",
      "2024-05-23 19:56:21,454 - INFO     - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "2024-05-23 19:56:21,455 - INFO     - model.text_field_embedder.token_embedders.tokens.model_name = roberta-large\n",
      "2024-05-23 19:56:21,455 - INFO     - model.text_field_embedder.token_embedders.tokens.max_length = None\n",
      "2024-05-23 19:56:21,455 - INFO     - model.text_field_embedder.token_embedders.tokens.sub_module = None\n",
      "2024-05-23 19:56:21,456 - INFO     - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
      "2024-05-23 19:56:21,456 - INFO     - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
      "2024-05-23 19:56:21,456 - INFO     - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
      "2024-05-23 19:56:21,457 - INFO     - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
      "2024-05-23 19:56:21,457 - INFO     - model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
      "2024-05-23 19:56:21,457 - INFO     - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
      "2024-05-23 19:56:21,458 - INFO     - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
      "2024-05-23 19:56:21,458 - INFO     - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
      "2024-05-23 19:56:21,458 - INFO     - model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
      "2024-05-23 19:56:21,705 - INFO     - removing temporary unarchived model dir at C:\\Users\\PIS054~1.PIN\\AppData\\Local\\Temp\\tmp82t24yl3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1535, in _get_module\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'transformers.models.align.configuration_align'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\pis05408.PINNACLE\\AppData\\Local\\Temp\\ipykernel_24332\\722857420.py\", line 2, in <module>\n",
      "    predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.02.17.tar.gz\")\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\predictors\\predictor.py\", line 366, in from_path\n",
      "    load_archive(archive_path, cuda_device=cuda_device, overrides=overrides),\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\models\\archival.py\", line 235, in load_archive\n",
      "    model = _load_model(config.duplicate(), weights_path, serialization_dir, cuda_device)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\models\\archival.py\", line 279, in _load_model\n",
      "    return Model.load(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\models\\model.py\", line 438, in load\n",
      "    return model_class._load(config, serialization_dir, weights_file, cuda_device)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\models\\model.py\", line 336, in _load\n",
      "    model = Model.from_params(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 604, in from_params\n",
      "    return retyped_subclass.from_params(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 636, in from_params\n",
      "    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 206, in create_kwargs\n",
      "    constructed_arg = pop_and_construct_arg(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 314, in pop_and_construct_arg\n",
      "    return construct_arg(class_name, name, popped_params, annotation, default, **extras)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 348, in construct_arg\n",
      "    result = annotation.from_params(params=popped_params, **subextras)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 604, in from_params\n",
      "    return retyped_subclass.from_params(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 636, in from_params\n",
      "    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 206, in create_kwargs\n",
      "    constructed_arg = pop_and_construct_arg(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 314, in pop_and_construct_arg\n",
      "    return construct_arg(class_name, name, popped_params, annotation, default, **extras)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 394, in construct_arg\n",
      "    value_dict[key] = construct_arg(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 348, in construct_arg\n",
      "    result = annotation.from_params(params=popped_params, **subextras)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 604, in from_params\n",
      "    return retyped_subclass.from_params(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 638, in from_params\n",
      "    return constructor_to_call(**kwargs)  # type: ignore\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\modules\\token_embedders\\pretrained_transformer_mismatched_embedder.py\", line 85, in __init__\n",
      "    self._matched_embedder = PretrainedTransformerEmbedder(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\modules\\token_embedders\\pretrained_transformer_embedder.py\", line 104, in __init__\n",
      "    self.transformer_model = cached_transformers.get(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\allennlp\\common\\cached_transformers.py\", line 180, in get\n",
      "    transformer = AutoModel.from_pretrained(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 540, in from_pretrained\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 751, in keys\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 752, in <listcomp>\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 748, in _load_attr_from_module\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 692, in getattribute_from_module\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1525, in __getattr__\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1537, in _get_module\n",
      "RuntimeError: Failed to import transformers.models.align.configuration_align because of the following error (look up to see its traceback):\n",
      "No module named 'transformers.models.align.configuration_align'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Suraj\\projects\\test\\env\\lib\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Load the sentiment analysis predictor\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.02.17.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment using AllenNLP\n",
    "def predict_sentiment_allennlp(text):\n",
    "    result = predictor.predict(sentence=text)\n",
    "    label = 'POSITIVE' if result['label'] == '1' else 'NEGATIVE'\n",
    "    score = result['probs'][int(result['label'])]\n",
    "    return {'label': label, 'score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment analysis function\n",
    "df['sentiment_allennlp'] = df['clean_email'].apply(predict_sentiment_allennlp)\n",
    "\n",
    "# Extract the sentiment label and score into separate columns\n",
    "df['sentiment_label_allennlp'] = df['sentiment_allennlp'].apply(lambda x: x['label'])\n",
    "df['sentiment_score_allennlp'] = df['sentiment_allennlp'].apply(lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
